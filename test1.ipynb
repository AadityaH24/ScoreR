{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix as cm, classification_report as cr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloaded MNIST Datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(X_train[0], cmap=\"gray\")\n",
    "plt.show()\n",
    "print (y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First two lines print the image. The last line shows the corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000, 28, 28, 1)\n",
    "X_test = X_test.reshape(10000, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaped data from (60000,28,28) to (60000,28,28,1) to be able to use in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converted labels to categorical format so that our model can predict these values categorically instead of continuously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Declare the model\n",
    "model = Sequential()\n",
    "\n",
    "## Declare the layers\n",
    "layer_1 = Conv2D(32, kernel_size=3, activation='relu', input_shape=(28, 28, 1))\n",
    "layer_2 = Conv2D(64, kernel_size=3, activation='relu')\n",
    "layer_3 = Flatten()\n",
    "layer_4 = Dense(10, activation='softmax')\n",
    "\n",
    "## Add the layers to the model\n",
    "model.add(layer_1)\n",
    "model.add(layer_2)\n",
    "model.add(layer_3)\n",
    "model.add(layer_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created the model with 4 layers. The first layer is a simple convolution layer that takes in inputs of shape (28,28,1) with the \"1\" indicating a greyscale image.\n",
    "\n",
    "The flatten layer allows us to link the convolution layer to the dense layer to obtain an output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have compiled the model using adam optimizer, as it is shown to be highly effective for categorical data. We will try to measure accuracy as a metric over the model's training stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 195s 104ms/step - loss: 0.4221 - accuracy: 0.9542 - val_loss: 0.0920 - val_accuracy: 0.9734\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 189s 101ms/step - loss: 0.0644 - accuracy: 0.9805 - val_loss: 0.0959 - val_accuracy: 0.9738\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 183s 97ms/step - loss: 0.0447 - accuracy: 0.9862 - val_loss: 0.1019 - val_accuracy: 0.9745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb8043473d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy went up from 95% to to 98.7% after 3 epochs. So we can stop training here to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/subramanian/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1813: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: digit/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"digit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Hard-maxed form of the prediction: \n",
      "\n",
      " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "\n",
      "--------- Prediction --------- \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Final Output: 0\n"
     ]
    }
   ],
   "source": [
    "example = X_train[1]\n",
    "prediction = model.predict(example.reshape(1, 28, 28, 1))\n",
    "\n",
    "hard_maxed_prediction = np.zeros(prediction.shape)\n",
    "hard_maxed_prediction[0][np.argmax(prediction)] = 1\n",
    "print (\"\\n\\nHard-maxed form of the prediction: \\n\\n {}\".format(hard_maxed_prediction))\n",
    "\n",
    "print (\"\\n\\n--------- Prediction --------- \\n\\n\")\n",
    "plt.imshow(example.reshape(28, 28), cmap=\"gray\")\n",
    "plt.show()\n",
    "print(\"\\n\\nFinal Output: {}\".format(np.argmax(prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our prediction matched the number in the image, so our model is trained fairly well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir1 in os.listdir('dataset2'):\n",
    "    if dir1!=\".DS_Store\":\n",
    "        for file in os.listdir(os.path.join('dataset2', dir1)):\n",
    "            image_path= os.path.join('dataset2', dir1,  file)\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "            img = 255 - image[:, :, 3]\n",
    "            x.append(img)\n",
    "            y.append(int(dir1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.15, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = xtrain.reshape(91570,28,28,1)\n",
    "xtest = xtest.reshape(16160,28,28,1)\n",
    "ytrain = to_categorical(ytrain)\n",
    "ytest = to_categorical(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Declare the model\n",
    "model2 = Sequential()\n",
    "\n",
    "## Declare the layers\n",
    "layer_1 = Conv2D(32, kernel_size=3, activation='relu', input_shape=(28, 28,1))\n",
    "layer_2 = Conv2D(64, kernel_size=3, activation='relu')\n",
    "layer_3 = Flatten()\n",
    "layer_4 = Dense(10, activation='softmax')\n",
    "\n",
    "## Add the layers to the model\n",
    "model2.add(layer_1)\n",
    "model2.add(layer_2)\n",
    "model2.add(layer_3)\n",
    "model2.add(layer_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2862/2862 [==============================] - 278s 97ms/step - loss: 1.1832 - accuracy: 0.9707 - val_loss: 0.0415 - val_accuracy: 0.9898\n",
      "Epoch 2/3\n",
      "2862/2862 [==============================] - 253s 88ms/step - loss: 0.0270 - accuracy: 0.9953 - val_loss: 0.0379 - val_accuracy: 0.9955\n",
      "Epoch 3/3\n",
      "2862/2862 [==============================] - 228s 80ms/step - loss: 0.0214 - accuracy: 0.9968 - val_loss: 0.0261 - val_accuracy: 0.9973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6e16580970>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(xtrain, ytrain, validation_data=(xtest, ytest), epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/subramanian/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1813: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: digit2/assets\n"
     ]
    }
   ],
   "source": [
    "model2.save(\"digit2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is trained on numbers that are encircled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fac6f2d9dc0>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = []\n",
    "y = []\n",
    "image = cv2.imread('circle_dataset/datadown/5/20250.png', cv2.IMREAD_UNCHANGED)\n",
    "img = 255 - image[:, :, 3]\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir1 in os.listdir('circle_dataset/datadown'):\n",
    "        for file in os.listdir(os.path.join('circle_dataset/datadown', dir1)):\n",
    "            image_path= os.path.join('circle_dataset/datadown', dir1,  file)\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "            img = 255 - image[:, :, 3]\n",
    "            x.append(img)\n",
    "            y.append(int(dir1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(x)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87480, 28, 28)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74358, 28, 28) (13122, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.15, random_state=101)\n",
    "print(xtrain.shape, xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_org = ytrain\n",
    "ytest_org = ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = xtrain.reshape(74358,28,28,1)\n",
    "xtest = xtest.reshape(13122,28,28,1)\n",
    "ytrain = to_categorical(ytrain)\n",
    "ytest = to_categorical(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Declare the model\n",
    "model3 = Sequential()\n",
    "\n",
    "## Declare the layers\n",
    "layer_1 = Conv2D(32, kernel_size=3, activation='relu', input_shape=(28, 28,1))\n",
    "layer_2 = Conv2D(16, kernel_size=3, activation='relu')\n",
    "layer_3 = Conv2D(12, kernel_size=3, activation ='relu')\n",
    "layer_4 = Flatten()\n",
    "layer_5 = Dense(10, activation='softmax')\n",
    "\n",
    "## Add the layers to the model\n",
    "model3.add(layer_1)\n",
    "model3.add(layer_2)\n",
    "model3.add(Dropout(0.2))\n",
    "model3.add(layer_3)\n",
    "model3.add(Dropout(0.2))\n",
    "model3.add(layer_4)\n",
    "model3.add(layer_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2324/2324 [==============================] - 246s 106ms/step - loss: 0.5307 - accuracy: 0.8913 - val_loss: 0.0041 - val_accuracy: 0.9989\n",
      "Epoch 2/2\n",
      "2324/2324 [==============================] - 246s 106ms/step - loss: 0.0244 - accuracy: 0.9930 - val_loss: 0.0032 - val_accuracy: 0.9989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fac71612d30>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(xtrain, ytrain, validation_data=(xtest, ytest), epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: digit3/assets\n"
     ]
    }
   ],
   "source": [
    "model3.save(\"digit3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.argmax(model3.predict(xtest), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1342\n",
      "           1       1.00      1.00      1.00      1322\n",
      "           2       1.00      1.00      1.00      1251\n",
      "           3       1.00      0.99      1.00      1320\n",
      "           4       1.00      1.00      1.00      1271\n",
      "           5       1.00      1.00      1.00      1387\n",
      "           6       1.00      1.00      1.00      1271\n",
      "           7       1.00      1.00      1.00      1310\n",
      "           8       1.00      1.00      1.00      1347\n",
      "           9       0.99      1.00      1.00      1301\n",
      "\n",
      "    accuracy                           1.00     13122\n",
      "   macro avg       1.00      1.00      1.00     13122\n",
      "weighted avg       1.00      1.00      1.00     13122\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cr(ytest_org, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1342    0    0    0    0    0    0    0    0    0]\n",
      " [   0 1322    0    0    0    0    0    0    0    0]\n",
      " [   0    0 1251    0    0    0    0    0    0    0]\n",
      " [   0    0    1 1308    0    1    0    2    0    8]\n",
      " [   0    0    0    0 1269    0    0    0    0    2]\n",
      " [   0    0    0    0    0 1387    0    0    0    0]\n",
      " [   0    0    0    0    0    0 1271    0    0    0]\n",
      " [   0    0    0    0    0    0    0 1310    0    0]\n",
      " [   0    0    0    0    0    0    0    0 1347    0]\n",
      " [   0    0    0    0    0    0    0    0    0 1301]]\n"
     ]
    }
   ],
   "source": [
    "print(cm(ytest_org, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
